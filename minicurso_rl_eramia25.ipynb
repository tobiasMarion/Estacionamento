{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tobiasMarion/Estacionamento/blob/main/minicurso_rl_eramia25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUofEn1oHUvj"
      },
      "source": [
        "# Aprendizado por ReforÃ§o: Como Ensinar RobÃ´s a Maximizar Recompensas na PrÃ¡tica\n",
        "\n",
        "Minicurso realizado no dia 12 de novembro durante o ERAMIA 2025 no Instituto de InformÃ¡tica da UFRGS.\n",
        "\n",
        "Autor: Lucas N. Alegre\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQIE4Aq4DaEN"
      },
      "source": [
        "## Modelando Problemas com Gymnasium\n",
        "\n",
        "Gymnasium Ã© a versÃ£o mantida do OpenAI Gym pela Farama Foundation. Ã‰ uma biblioteca Python para desenvolvimento e comparaÃ§Ã£o de algoritmos de Reinforcement Learning.\n",
        "\n",
        "### InstalaÃ§Ã£o\n",
        "\n",
        "```bash\n",
        "pip install gymnasium\n",
        "pip install gymnasium[all]  # Para ambientes adicionais\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-IXG_svtHReK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Cwob9wbCDaEP"
      },
      "outputs": [],
      "source": [
        "# @title Definindo um Agente AleatÃ³rio\n",
        "class RandomAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "    def eval(self, obs):\n",
        "        return self.env.action_space.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyi_GN6IDaEQ"
      },
      "outputs": [],
      "source": [
        "## 1. Estrutura BÃ¡sica de um Ambiente\n",
        "\n",
        "# Criar ambiente\n",
        "# https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
        "def make_env():\n",
        "    return gym.make(\n",
        "                'FrozenLake-v1',\n",
        "                desc=None,\n",
        "                map_name=\"4x4\",\n",
        "                is_slippery=True,\n",
        "                success_rate=1.0/3.0,\n",
        "                reward_schedule=(1, 0, 0),\n",
        "                render_mode=\"rgb_array\"\n",
        "            )\n",
        "\n",
        "env = make_env()\n",
        "\n",
        "# Observation Space\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "print(f\"NÃºmero de estados: {env.observation_space.n}\")\n",
        "\n",
        "# Action Space\n",
        "print(f\"\\nAction space: {env.action_space}\")\n",
        "print(f\"NÃºmero de aÃ§Ãµes: {env.action_space.n}\")\n",
        "print(f\"AÃ§Ãµes: 0=Esquerda, 1=Baixo, 2=Direita, 3=Cima\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUKne4kCDaEQ"
      },
      "outputs": [],
      "source": [
        "# Resetar ambiente\n",
        "observation, info = env.reset(seed=42)\n",
        "\n",
        "agent = RandomAgent(env)\n",
        "\n",
        "print(f\"Observation: {observation}\")\n",
        "print(f\"Info: {info}\")\n",
        "\n",
        "# Loop de interaÃ§Ã£o\n",
        "for _ in range(100):\n",
        "    # Escolher aÃ§Ã£o (aleatÃ³ria neste exemplo)\n",
        "    action = agent.eval(observation)\n",
        "\n",
        "    # Executar aÃ§Ã£o\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    # Verificar se episÃ³dio terminou\n",
        "    if terminated or truncated:\n",
        "        print(f\"EpisÃ³dio terminou! Recompensa: {reward}\")\n",
        "        observation, info = env.reset()\n",
        "        break\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5JPZzTPI2oQ"
      },
      "outputs": [],
      "source": [
        "# @title Animando o agente\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "def animate_agent(agent, env, num_frames=100):\n",
        "  s, info = env.reset()\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "  im = axes[0].imshow(env.render())\n",
        "  frames = [env.render()]\n",
        "  returns = [0]\n",
        "  env_active = True\n",
        "  for step in range(num_frames):\n",
        "    a = agent.eval(s)\n",
        "    s, r, terminated, truncated, info = env.step(a)\n",
        "    done = terminated or truncated\n",
        "    frames.append(env.render())\n",
        "    returns.append(r + returns[-1])\n",
        "    if env_active and done:\n",
        "      env_active = False\n",
        "      print(f'Game over! Your agent lasted {step} steps.')\n",
        "  axes[1].set_title('Cumulative returns', fontsize=20)\n",
        "  axes[1].set_xlim(0, num_frames)\n",
        "  axes[1].set_ylim(0, max(returns) * 1.2)\n",
        "  line, = axes[1].plot([], [], lw=2)\n",
        "\n",
        "  def init():\n",
        "    line.set_data([], [])\n",
        "    im.set_data(frames[0])\n",
        "    return [im]\n",
        "\n",
        "  def animate(i):\n",
        "    line.set_data(np.arange(i), returns[:i])\n",
        "    im.set_data(frames[i])\n",
        "    return [im]\n",
        "\n",
        "  anim = animation.FuncAnimation(fig, animate, init_func=init, frames=num_frames,\n",
        "                                 interval=50)\n",
        "  plt.close()\n",
        "  return HTML(anim.to_jshtml())\n",
        "\n",
        "\n",
        "def eval_agent(agent, env) -> float:\n",
        "    \"\"\"Evaluate the agent for one episode and return the total reward.\"\"\"\n",
        "    s, info = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    gamma = 1.0\n",
        "    while not done:\n",
        "        a = agent.eval(s)\n",
        "        s, r, terminated, truncated, info = env.step(a)\n",
        "        done = terminated or truncated\n",
        "        total_reward += r * gamma\n",
        "        gamma *= agent.gamma\n",
        "    return total_reward\n",
        "\n",
        "def average_eval_agent(agent, env, num_episodes=10) -> float:\n",
        "    \"\"\"Evaluate the agent for multiple episodes and return the average reward.\"\"\"\n",
        "    total_rewards = 0\n",
        "    for _ in range(num_episodes):\n",
        "        total_rewards += eval_agent(agent, env)\n",
        "    return total_rewards / num_episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlHg49rUDaEQ"
      },
      "outputs": [],
      "source": [
        "animate_agent(agent, env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrddzeYoDaER"
      },
      "source": [
        "### Recursos Adicionais\n",
        "\n",
        "- ðŸ“š DocumentaÃ§Ã£o oficial: https://gymnasium.farama.org/\n",
        "- ðŸŽ® Lista de ambientes: https://gymnasium.farama.org/environments/\n",
        "- ðŸ’» GitHub: https://github.com/Farama-Foundation/Gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEjrXq41IMVt"
      },
      "source": [
        "# Q-Learning Tabular\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e2wdnCuDaER"
      },
      "outputs": [],
      "source": [
        "class QLearning:\n",
        "\n",
        "    def __init__(self, env, eval_env, learning_rate=0.1, gamma=0.99, exploration_rate=1.0, exploration_decay=0.995, min_exploration_rate=0.01):\n",
        "        self.state_size = env.observation_space.n\n",
        "        self.action_size = env.action_space.n\n",
        "        self.eval_env = eval_env\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = gamma\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "        self.min_exploration_rate = min_exploration_rate\n",
        "        self.q_table = np.zeros((self.state_size, self.action_size))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
        "        # COMPLETE AQUI\n",
        "\n",
        "    def eval(self, state):\n",
        "        \"\"\"Greedy action selection.\"\"\"\n",
        "        # COMPLETE AQUI\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        best_next_action = np.argmax(self.q_table[next_state])\n",
        "\n",
        "        td_target = # COMPLETE AQUI\n",
        "        td_error = # COMPLETE AQUI\n",
        "        self.q_table[state][action] += self.learning_rate * td_error\n",
        "\n",
        "        if done:\n",
        "            self.exploration_rate = max(self.min_exploration_rate, self.exploration_rate * self.exploration_decay)\n",
        "\n",
        "    def train(self, env, num_steps):\n",
        "        eval_rewards = []\n",
        "        state, _ = env.reset()\n",
        "        for step in range(num_steps):\n",
        "            action = self.choose_action(state)\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            self.learn(state, action, reward, next_state, terminated)\n",
        "\n",
        "            state = next_state\n",
        "            if terminated or truncated:\n",
        "                state, _ = env.reset()\n",
        "\n",
        "            if step % 10000 == 0:\n",
        "                eval_reward = average_eval_agent(self, self.eval_env)\n",
        "                eval_rewards.append(eval_reward)\n",
        "                print(f\"Step: {step}, Eval Reward: {eval_reward}, Exploration Rate: {self.exploration_rate:.4f}\")\n",
        "\n",
        "        return eval_rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZpaM1kDDaER"
      },
      "outputs": [],
      "source": [
        "# https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
        "def make_env(render_mode=None):\n",
        "    return gym.make(\n",
        "                'FrozenLake-v1',\n",
        "                desc=None,\n",
        "                map_name=\"4x4\",\n",
        "                is_slippery=True,\n",
        "                success_rate=0.9,\n",
        "                reward_schedule=(1, 0, 0),\n",
        "                render_mode=render_mode\n",
        "            )\n",
        "env = make_env()\n",
        "eval_env = make_env(render_mode='rgb_array')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PKqH65KDaER"
      },
      "outputs": [],
      "source": [
        "agent1 = QLearning(env, eval_env, exploration_decay=0.9999, learning_rate=0.5)\n",
        "eval_rewards1 = agent1.train(env, num_steps=1000000)\n",
        "\n",
        "agent2 = QLearning(env, eval_env, exploration_decay=0.9999, learning_rate=0.1)\n",
        "eval_rewards2 = agent2.train(env, num_steps=1000000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GB_Oq2ODaES"
      },
      "outputs": [],
      "source": [
        "agent1.q_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "py-jAs0yDaES"
      },
      "outputs": [],
      "source": [
        "animate_agent(agent1, eval_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF_fx9A0DaES"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(eval_rewards1, label='Agent 1')\n",
        "plt.plot(eval_rewards2, label='Agent 2')\n",
        "plt.xlabel('Evaluation Step (x10,000)')\n",
        "plt.ylabel('Evaluation Reward')\n",
        "plt.title('Q-Learning: Evaluation Reward Curves')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhB5TbrdJ18W"
      },
      "source": [
        "## Deep Q-Networks (DQN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wtkr-1zLDaES"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Type\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzmuzL56DaES"
      },
      "source": [
        "### Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KH6sMSzDDaES"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ReplayBufferSamples:\n",
        "    \"\"\"\n",
        "    A dataclass containing transitions from the replay buffer.\n",
        "    \"\"\"\n",
        "    observations: np.ndarray  # same as states in the theory\n",
        "    next_observations: np.ndarray\n",
        "    actions: np.ndarray\n",
        "    rewards: np.ndarray\n",
        "    terminateds: np.ndarray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8VfsHwfDaES"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    A simple replay buffer class to store and sample transitions.\n",
        "\n",
        "    :param buffer_size: Max number of transitions to store\n",
        "    :param observation_space: Observation space of the env,\n",
        "        contains information about the observation type and shape.\n",
        "    :param action_space: Action space of the env,\n",
        "        contains information about the number of actions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        buffer_size: int,\n",
        "        observation_space: spaces.Box,\n",
        "        action_space: spaces.Discrete,\n",
        "    ) -> None:\n",
        "        # Current position in the ring buffer\n",
        "        self.current_idx = 0\n",
        "        self.buffer_size = buffer_size\n",
        "        # Boolean flag to know when the buffer has reached its maximal capacity\n",
        "        self.is_full = False\n",
        "\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "        # Create the different buffers\n",
        "        self.observations = np.zeros((buffer_size, *observation_space.shape), dtype=observation_space.dtype)\n",
        "        self.next_observations = np.zeros((buffer_size, *observation_space.shape), dtype=observation_space.dtype)\n",
        "        # The action is an integer\n",
        "        action_dim = 1\n",
        "        self.actions = np.zeros((buffer_size, action_dim), dtype=action_space.dtype)\n",
        "\n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "        # TODO: create the buffers (numpy arrays) for the rewards (dtype=np.float32)\n",
        "        # and the terminated signals (dtype=bool)\n",
        "\n",
        "\n",
        "        ### END OF YOUR CODE\n",
        "\n",
        "    def store_transition(\n",
        "        self,\n",
        "        obs: np.ndarray,\n",
        "        next_obs: np.ndarray,\n",
        "        action: int,\n",
        "        reward: float,\n",
        "        terminated: bool,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Store one transition in the buffer.\n",
        "\n",
        "        :param obs: Current observation\n",
        "        :param next_obs: Next observation\n",
        "        :param action: Action taken for the current observation\n",
        "        :param reward: Reward received after taking the action\n",
        "        :param terminated: Whether it is the end of an episode or not\n",
        "            (discarding episode truncation like timeout)\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "        # TODO:\n",
        "        # 1. Update the different buffers defined in the __init__\n",
        "        # 2. Update the pointer (`self.current_idx`), careful\n",
        "        # the pointer need to be set to zero when reaching the end of the ring buffer\n",
        "\n",
        "        # Update the buffers to store the new transition\n",
        "\n",
        "\n",
        "        # Update the pointer\n",
        "        self.current_idx += 1\n",
        "        # If the buffer is full, we start from zero again, this is a ring buffer\n",
        "        # you also need to set the flag `is_full` to True (so we know the buffer has reached its max capacity)\n",
        "\n",
        "\n",
        "        ### END OF YOUR CODE\n",
        "\n",
        "    def sample(self, batch_size: int) -> ReplayBufferSamples:\n",
        "        \"\"\"\n",
        "        Sample with replacement `batch_size` transitions from the buffer.\n",
        "\n",
        "        :param batch_size: How many transitions to sample.\n",
        "        :return: Samples from the replay buffer\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. Retrieve the upper bound (max index that can be sampled)\n",
        "        #  it corresponds to `self.buffer_size` when the ring buffer is full (we can samples all indices)\n",
        "        # 2. Sample `batch_size` indices with replacement from the buffer\n",
        "        # (in the range [0, upper_bound[ ), numpy has a method `np.random.randint` for that ;)\n",
        "        upper_bound = self.buffer_size if self.is_full else self.current_idx\n",
        "        batch_indices = np.random.randint(0, upper_bound, size=batch_size)\n",
        "\n",
        "        return ReplayBufferSamples(\n",
        "            self.observations[batch_indices],\n",
        "            self.next_observations[batch_indices],\n",
        "            self.actions[batch_indices],\n",
        "            self.rewards[batch_indices],\n",
        "            self.terminateds[batch_indices],\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlVGb4A4DaES"
      },
      "source": [
        "### Q Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UgZgg4xDaES"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A Q-Network for the DQN algorithm\n",
        "    to estimate the q-value for a given observation.\n",
        "\n",
        "    :param observation_space: Observation space of the env,\n",
        "        contains information about the observation type and shape.\n",
        "    :param action_space: Action space of the env,\n",
        "        contains information about the number of actions.\n",
        "    :param n_hidden_units: Number of units for each hidden layer.\n",
        "    :param activation_fn: Activation function (ReLU by default)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_space: spaces.Box,\n",
        "        action_space: spaces.Discrete,\n",
        "        n_hidden_units: int = 64,\n",
        "        activation_fn: Type[nn.Module] = nn.ReLU,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        # Assume 1d space\n",
        "        obs_dim = observation_space.shape[0]\n",
        "\n",
        "        # 1. Retrieve the number of discrete actions,\n",
        "        # that will be the number of ouputs of the q-network\n",
        "        # 2. Create the q-network, it will be a two layers fully-connected\n",
        "        # neural network which take the state (observation) as input\n",
        "        # and outputs the q-values for all possible actions\n",
        "\n",
        "        # Retrieve the number of discrete actions (using attribute `n` from `action_space`)\n",
        "        n_actions = int(action_space.n)\n",
        "\n",
        "        # Create the q network: a 2 fully connected hidden layers with `n_hidden_units` each\n",
        "        # with `activation_fn` for the activation function after each hidden layer.\n",
        "        # You should use `nn.Sequential` (combine several layers to create a network)\n",
        "        # `nn.Linear` (fully connected layer) from PyTorch.\n",
        "        self.q_net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, n_hidden_units),\n",
        "            activation_fn(),\n",
        "            nn.Linear(n_hidden_units, n_hidden_units),\n",
        "            activation_fn(),\n",
        "            nn.Linear(n_hidden_units, n_actions),\n",
        "        )\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        \"\"\"\n",
        "        :param observations: A batch of observation (batch_size, obs_dim)\n",
        "        :return: The Q-values for the given observations\n",
        "            for all the action (batch_size, n_actions)\n",
        "        \"\"\"\n",
        "        return self.q_net(observations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjQwP8OmDaES"
      },
      "source": [
        "### DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvL-N2JhDaES"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    A DQN agent implementation.\n",
        "\n",
        "    Mnih, V., Kavukcuoglu, K., Silver, D. et al. Human-level control through deep reinforcement learning. Nature 518, 529â€“533 (2015).\n",
        "    https://doi.org/10.1038/nature14236\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        eval_env,\n",
        "        n_hidden_units=256,\n",
        "        learning_rate=1e-3,\n",
        "        gamma=0.99,\n",
        "        buffer_size=200000,\n",
        "        batch_size=256,\n",
        "        target_update_freq=1000,\n",
        "        device=\"auto\",\n",
        "    ):\n",
        "        self.env = env\n",
        "        self.eval_env = eval_env\n",
        "        self.action_space = env.action_space\n",
        "        self.observation_space = env.observation_space\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.target_update_freq = target_update_freq\n",
        "        self.device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\") if device == \"auto\" else device\n",
        "\n",
        "        self.q_net = QNetwork(self.observation_space, self.action_space, n_hidden_units).to(device)\n",
        "        self.target_q_net = QNetwork(self.observation_space, self.action_space, n_hidden_units).to(device)\n",
        "        self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
        "        self.optimizer = th.optim.Adam(self.q_net.parameters(), lr=learning_rate)\n",
        "\n",
        "        self.replay_buffer = ReplayBuffer(\n",
        "            buffer_size=buffer_size,\n",
        "            observation_space= self.observation_space,\n",
        "            action_space= self.action_space\n",
        "        )\n",
        "        self.learn_step = 0\n",
        "\n",
        "    def select_action(self, state, epsilon=0.05):\n",
        "        if np.random.rand() < epsilon:\n",
        "            return self.action_space.sample()\n",
        "        state_tensor = th.tensor(state, dtype=th.float32, device=self.device).unsqueeze(0)\n",
        "        with th.no_grad():\n",
        "            q_values = self.q_net(state_tensor)\n",
        "        return int(q_values.argmax().item())\n",
        "\n",
        "    def store_transition(self, obs, next_obs, action, reward, terminated):\n",
        "        self.replay_buffer.store_transition(obs, next_obs, action, reward, terminated)\n",
        "\n",
        "    def train_step(self):\n",
        "        if self.replay_buffer.current_idx < self.batch_size and not self.replay_buffer.is_full:\n",
        "            return\n",
        "\n",
        "        samples = self.replay_buffer.sample(self.batch_size)\n",
        "        obs = th.tensor(samples.observations, dtype=th.float32, device=self.device)\n",
        "        next_obs = th.tensor(samples.next_observations, dtype=th.float32, device=self.device)\n",
        "        actions = th.tensor(samples.actions, dtype=th.int64, device=self.device).squeeze(-1)\n",
        "        rewards = th.tensor(samples.rewards, dtype=th.float32, device=self.device)\n",
        "        dones = th.tensor(samples.terminateds, dtype=th.float32, device=self.device)\n",
        "\n",
        "        q_values = self.q_net(obs).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        with th.no_grad():\n",
        "            # Double DQN target calculation\n",
        "            next_actions = # COMPLETE AQUI\n",
        "            next_q_values = self.target_q_net(next_obs).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
        "            target = # COMPLETE AQUI\n",
        "\n",
        "        loss = nn.functional.mse_loss(q_values, target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.learn_step += 1\n",
        "        # Udpate target network\n",
        "        if self.learn_step % self.target_update_freq == 0:\n",
        "            self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "    def eval(self, state):\n",
        "        state_tensor = th.tensor(state, dtype=th.float32, device=self.device).unsqueeze(0)\n",
        "        with th.no_grad():\n",
        "            q_values = self.q_net(state_tensor)\n",
        "        return int(q_values.argmax().item())\n",
        "\n",
        "    def train(self, num_steps, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.999):\n",
        "        epsilon = epsilon_start\n",
        "        eval_rewards = []\n",
        "\n",
        "        state, _ = self.env.reset()\n",
        "        for step in range(num_steps):\n",
        "\n",
        "            action = self.select_action(state, epsilon)\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "\n",
        "            self.store_transition(state, next_state, action, reward, terminated)\n",
        "\n",
        "            self.train_step()\n",
        "\n",
        "            state = next_state\n",
        "            if terminated or truncated:\n",
        "                state, _ = self.env.reset()\n",
        "\n",
        "            if step % 1000 == 0:\n",
        "                eval_reward = average_eval_agent(self, self.eval_env)\n",
        "                eval_rewards.append(eval_reward)\n",
        "                print(f\"Step: {step}, Eval Reward: {eval_reward}\")\n",
        "\n",
        "            epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
        "\n",
        "        return eval_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72twD6m0DaET"
      },
      "source": [
        "## Training the Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_cNV8O0DaET"
      },
      "outputs": [],
      "source": [
        "@title Ambiente Fetch com AÃ§Ãµes Discretas e ObservaÃ§Ãµes Modificadas\n",
        "\n",
        "import gymnasium_robotics\n",
        "from gymnasium.core import ActionWrapper, ObservationWrapper\n",
        "from gymnasium.spaces import Discrete\n",
        "\n",
        "class FetchObservationWrapper(ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.observation_space=gym.spaces.Box(\n",
        "            low=np.concatenate((env.unwrapped.observation_space['observation'].low, env.unwrapped.observation_space['desired_goal'].low)),\n",
        "            high=np.concatenate((env.unwrapped.observation_space['observation'].high, env.unwrapped.observation_space['desired_goal'].high)),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # concatenate observation and desired_goal to form a single observation\n",
        "        return np.hstack((observation[\"observation\"], observation[\"desired_goal\"]))\n",
        "\n",
        "\n",
        "class FetchDiscreteManhattanAction(ActionWrapper):\n",
        "    def __init__(self, env, use_gripper=False, use_null_action=False):\n",
        "        super().__init__(env)\n",
        "        self.use_gripper = use_gripper\n",
        "        self.use_null_action = use_null_action\n",
        "\n",
        "        self.action_dict = {\n",
        "            0: np.array([1.0, 0.0, 0.0, -1.0], dtype=np.float32),\n",
        "            1: np.array([-1.0, 0.0, 0.0, -1.0], dtype=np.float32),\n",
        "            2: np.array([0.0, 1.0, 0.0, -1.0], dtype=np.float32),\n",
        "            3: np.array([0.0, -1.0, 0.0, -1.0], dtype=np.float32),\n",
        "            4: np.array([0.0, 0.0, 1.0, -1.0], dtype=np.float32),\n",
        "            5: np.array([0.0, 0.0, -1.0, -1.0], dtype=np.float32)\n",
        "        }\n",
        "        self.num_actions = 6\n",
        "        if self.use_gripper:\n",
        "            self.action_dict[self.num_actions] = np.array([0.0, 0.0, 0.0, 1.0], dtype=np.float32)\n",
        "            self.action_dict[self.num_actions + 1] = np.array([0.0, 0.0, 0.0, -1.0], dtype=np.float32)\n",
        "            self.num_actions += 2\n",
        "        if self.use_null_action:\n",
        "            self.action_dict[self.num_actions] = np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float32)\n",
        "            self.num_actions += 1\n",
        "\n",
        "        self.action_space = Discrete(self.num_actions)\n",
        "\n",
        "    def action(self, action):\n",
        "        real_action = self.action_dict[int(action)]\n",
        "        return real_action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViRHGp9BDaET"
      },
      "outputs": [],
      "source": [
        "def make_env(render_mode=None):\n",
        "    env = gym.make(\n",
        "        \"FetchReachDense-v4\",\n",
        "        render_mode=render_mode,\n",
        "    )\n",
        "    env = FetchObservationWrapper(env)\n",
        "    env = FetchDiscreteManhattanAction(env, use_gripper=False, use_null_action=True)\n",
        "    return env\n",
        "\n",
        "env = make_env()\n",
        "eval_env = make_env(render_mode='rgb_array')\n",
        "\n",
        "agent = DQNAgent(\n",
        "    env,\n",
        "    eval_env,\n",
        "    n_hidden_units=128,\n",
        "    learning_rate=1e-3,\n",
        "    buffer_size=50000,\n",
        "    batch_size=64,\n",
        "    target_update_freq=1000,\n",
        "    device=\"cpu\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60uaQO76DaET"
      },
      "outputs": [],
      "source": [
        "agent.train(num_steps=50000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeZVMoDeDaET"
      },
      "outputs": [],
      "source": [
        "animate_agent(agent, eval_env, num_frames=10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "minicurso-rl-eramia25",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}